{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UTS_ML2019_A2_.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahirurane/UTS_ML2019_A2/blob/master/UTS_ML2019_A2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md5Y4KzNRHT2",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_2hhxKJOaOU",
        "colab_type": "text"
      },
      "source": [
        "##DATA ANALYTICS PROJECT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_PGIBUIPLEe",
        "colab_type": "text"
      },
      "source": [
        "###**BUSINENESS PROBLEM**\n",
        "\n",
        "E-commerce has boomed exponentially from 2014 to 2020 by 200%. With increase of sale, companies are able to reach customers globally. Companies started facing issues in handling customer complaint. The companies must analyse millions of feedback which is humanly impossible. The companies like Amazon, Flipkart started using Text analysis technique to analyse customer’s feedback. The technique helps to analyse feedbacks and find out whether customer are positive or negative about the products. It is critical to evaluate the customers’ reviews as it will decide the organizations’ business decision. That’s why, it is very critical to get customer feedback, clean it, analyse it and visualize it. The visualization should be in a way that non-technical professions can understand it. Once data is visualized, the organization can re-evaluate the business strategies. After changing the business strategies, again the engineer collects text data and runs through same cycle. By doing that, the company can continuously update the product based on customers’ negative feedback.\n",
        "\n",
        "It is required to define the business before defining business problem. E-Commerce organization like Amazon, Flipkart sell products of all brands. Amazon sell their own Amazon certified products as well. On the website, it allows users to provide feedback. Based on the feedback, it improvise the products. Our aim in the report is to use sentiment analysis on customers’ feedback by working on data uploaded on Kaggle. We chose to work on text analysis as we have seen that by reviewing text analysis, small and medium companies have improved the service/product. It can be done for giant company like Amazon as well. The sentimental analysis will give big boost and eliminate the customer’s pain. Here, we are focusing on Amazon's Echo product. \n",
        "\n",
        "Definition of Sentimental Analysis: \n",
        "\n",
        "It is a method to analysis text data by converting in a data such that machine can understand the language and gives meaningful visualization. The visualization is then can be helpful for choosing the strategies.\n",
        "\n",
        "Thus the problem statement is: 'Sentimental analysis on Amazon Product using data extracted Amazon feedback.' The statement is broken into sub-business problem as written below:\n",
        "\n",
        "\n",
        "\n",
        "This analysis will help the business heads in analyzing customer's feeling for Amazon Echo. It can be useful when Amazon wants to launch same product with upgraded features, it will be helpful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T_9IocUV7JG",
        "colab_type": "text"
      },
      "source": [
        "###DATA MINING PROBLEM\n",
        "\n",
        "  Predictive Analysis, Machine Learning, Data Mining, and Big Data techniques have been adopted to analyse and classify the tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHlkKrNPXBCb",
        "colab_type": "code",
        "outputId": "6cc94558-5e24-4afd-83a3-48170b855820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "Image('Framework.jpg', width =525)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "Framework.jpg",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 525
            }
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq7M9w_tXcba",
        "colab_type": "text"
      },
      "source": [
        "The Data Analysis part comprises of two phases:\n",
        "\n",
        "\n",
        "1.   Data Cleaning Phase: The data which we got from Kaggle (https://www.kaggle.com/sid321axn/amazon-alexa-reviews) has been cleaned. This part will be discussed in the report later.\n",
        "2.   Predictive Analysis: To analyze and predict dataset accurately , we will make a model which helps us to visualize the data. The figure of the model is shown as below in the figure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_1cvUKTZlyt",
        "colab_type": "code",
        "outputId": "63f5d6a7-a960-4e09-86b3-275536d0e90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Image('Steps.png', width =525)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "Steps.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 525
            }
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17PxwVVZZ1E2",
        "colab_type": "text"
      },
      "source": [
        "##'Python' Programming Language\n",
        "\n",
        "It is an open source programming language, being used in industry for text/data analysis. We will be using Python through out the model. Next section, we will explain the codes. \n",
        "\n",
        "Below are the libraries being used:\n",
        "\n",
        "\n",
        "1.   Pandas: Library is used for data read, write ,and manipulation.\n",
        "2.   numpy: It is general purpose array processing package.\n",
        "4.   Seaborn: Plotting a graph and visualization in the report.\n",
        "5.   WordCloud: For displaying wordcloud in later part of the section\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEilLG0oT3BB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score , confusion_matrix\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2opKKVPT4FI",
        "colab_type": "code",
        "outputId": "3507a9ec-e4d6-480a-cb90-a69bc573e5a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/datasets"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-48-47fdefcad9af>\", line 3, in <module>\n",
            "    get_ipython().magic('cd /gdrive/My Drive/datasets')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
            "    return self.run_line_magic(magic_name, magic_arg_s)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
            "    result = fn(*args,**kwargs)\n",
            "  File \"</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-91>\", line 2, in cd\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/osm.py\", line 288, in cd\n",
            "    oldcwd = py3compat.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w9q-iz1k9zE",
        "colab_type": "text"
      },
      "source": [
        "The above code will mount the Google Drive where the dataset extracted from Kaggle. It asks for authorization code. We need to click on the URL mentioned in the output and allow Colab to access Google Drive. Once authorization is given, copy the code and it will load the data in Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaBvWmhnGILq",
        "colab_type": "text"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFKkGRXvGJTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('./amazon_alexa.tsv', delimiter='\\t')\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k-G8yY7IESZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['review_item_length'] = dataset.verified_reviews.apply(lambda x: len(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BYOqUOuIqbd",
        "colab_type": "text"
      },
      "source": [
        "Function to remove patters from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9l1IVrfIUEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "        \n",
        "    return input_txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPXxl8GAI4vL",
        "colab_type": "text"
      },
      "source": [
        "punctuations, numbers and special characters do not help much"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Djyj8ZlIvvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove special characters, numbers, punctuations\n",
        "dataset['review_clear'] = dataset['verified_reviews'].str.replace(\"[^a-zA-Z#]\", \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9no5QG6NN7W",
        "colab_type": "text"
      },
      "source": [
        "Lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDMaE-cNNO3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['review_clear'] = dataset.review_clear.apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CzvnN00Ndep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The mean for the length of review:',dataset['review_item_length'].mean())\n",
        "print('The standard deviation for the length of reviews:',dataset['review_item_length'].std())\n",
        "print('The maximum for the length of reviews:',dataset['review_item_length'].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLddq-wzOLcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['review_item_length'].hist(bins=20)\n",
        "plt.title('Distribution of review text length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCt84mUc8a7B",
        "colab_type": "text"
      },
      "source": [
        "Convert Data format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_LDKkahO6C0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(321)\n",
        "sns.set(rc={'figure.figsize':(14,8)})\n",
        "reviews = ' '.join(dataset['review_clear'].tolist())\n",
        "\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(reviews)\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.title('Reviews',size=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28C8VN2DPiWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(rc={'figure.figsize':(10,6)})\n",
        "sns.countplot(dataset.variation,\n",
        "              order = dataset['variation'].value_counts().index)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Counts of each variation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWxULlhCP5I4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.rating.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhQDl9-TQA2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data5 = dataset[dataset.rating == 5]\n",
        "data_not_5 = dataset[dataset.rating != 5]\n",
        "data1 = dataset[dataset.rating == 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyr5SjLqQJjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(rc={'figure.figsize':(14,8)})\n",
        "reviews = ' '.join(data5['verified_reviews'].tolist())\n",
        "\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(reviews)\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.title('Reviews of rating 5',size=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyU3ytW0QPTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(rc={'figure.figsize':(14,8)})\n",
        "reviews = ' '.join(data_not_5['verified_reviews'].tolist())\n",
        "\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(reviews)\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.title('Reviews Lower than 5',size=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYvVZToRQZWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.boxplot(dataset.variation, dataset.rating)\n",
        "plt.xticks(rotation = 90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDTLKgLmS5cY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['positive'] = 0\n",
        "dataset.loc[dataset['rating'] ==5, 'positive'] = 1\n",
        "\n",
        "y = dataset['positive']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixA0xnSrSM73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "word_tokenize(dataset.verified_reviews[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl6oO0HLSo7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "#Tokenize words\n",
        "dataset['review_clear'] = dataset.verified_reviews.apply(lambda x: word_tokenize(x))\n",
        "\n",
        "#remove stop words \n",
        "dataset['review_clear'] = dataset.review_clear.apply(lambda x: [w for w in x if w not in stop_words])\n",
        "\n",
        "ps = PorterStemmer() \n",
        "  \n",
        "# choose some words to be stemmed \n",
        "dataset['review_clear'] = dataset.review_clear.apply(lambda x: [ps.stem(w) for w in x])\n",
        " \n",
        "\n",
        "#append sentence\n",
        "dataset['review_clear'] = dataset.review_clear.apply(lambda x: ' '.join(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2ErDbctTBLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UnfcIgnTRaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSi2DfbnVqKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "# creating bag of words for training bag-of-words feature matrix\n",
        "bagOfWords = bow_vectorizer.fit_transform(dataset['review_clear'])\n",
        "print(\"bow\",bagOfWords)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7dhyViAXUGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "# # TF-IDF feature matrix\n",
        "# tfidf = tfidf_vectorizer.fit_transform(dataset['review_clear'])\n",
        "# print(\"tfidf\",tfidf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdEtBlaKAZlr",
        "colab_type": "text"
      },
      "source": [
        "Split Dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFNwYM1EAbin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(bagOfWords, y, test_size=0.33\n",
        "                                    ,random_state=53)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHit2lXrXYFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting data into training and validation set\n",
        "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(X_train, y_train, random_state=42, test_size=0.3)\n",
        "\n",
        "lreg = LogisticRegression()\n",
        "lreg.fit(xtrain_bow, ytrain) # training the model\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
        "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "f1_score(yvalid, prediction_int) # calculating f1 score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byXrD3jjhqOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = lreg.predict_proba(X_test)\n",
        "test_pred_int = test_pred[:,1] >= 0.3\n",
        "test_pred_int = test_pred_int.astype(np.int)\n",
        "\n",
        "score = accuracy_score(y_test, test_pred_int)\n",
        "print('Accuracy is:',score)\n",
        "f1 = f1_score(y_test, test_pred_int)\n",
        "print('F score is:',f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6Hnr9ZHCNsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.heatmap(confusion_matrix(y_test,test_pred_int), annot=True,fmt='2.0f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNHBc9M5QEZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xg_reg = xgb.XGBRegressor()\n",
        "\n",
        "xg_reg.fit(xtrain_bow, ytrain)\n",
        "\n",
        "xg_preds = xg_reg.predict(xvalid_bow)\n",
        "\n",
        "predictions = [round(value) for value in xg_preds]\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(yvalid, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIhfNVtviEIW",
        "colab_type": "text"
      },
      "source": [
        "https://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dksIl1fMew3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = MultinomialNB()\n",
        "clf.fit(xtrain_bow, ytrain)\n",
        "nb_preds = clf.predict(xvalid_bow)\n",
        "f1_score(yvalid, nb_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekhcrno2ZkA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = svm.SVC(gamma='scale')\n",
        "clf.fit(xtrain_bow, ytrain)  \n",
        "svm_predict = clf.predict(xvalid_bow)\n",
        "\n",
        "f1_score(yvalid, svm_predict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPIdRD5_nXUO",
        "colab_type": "text"
      },
      "source": [
        "## Analysis using Lexicon and Rule Based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLifyGkJn1tX",
        "colab_type": "text"
      },
      "source": [
        "Import Lexcion Library 'vaderSentiment'\n",
        "(https://github.com/cjhutto/vaderSentiment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1k7u0cJnqYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install vaderSentiment\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKgPh6XyoWjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyser = SentimentIntensityAnalyzer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_allGJHMn9sB",
        "colab_type": "text"
      },
      "source": [
        "Defining Score method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WqzWJ9Cnrxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentiment_analyzer_scores(sentence):\n",
        "    score = analyser.polarity_scores(sentence)\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTMjEvIoBJ5",
        "colab_type": "text"
      },
      "source": [
        "Calcualting score for review comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e80BXw1lnxO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = []\n",
        "y = []\n",
        "\n",
        "for review_comment in dataset.review_clear:\n",
        "  item = sentiment_analyzer_scores(review_comment)\n",
        "  \n",
        "  x.append(item['neg'])\n",
        "  y.append(item['pos'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BSGdaPToG9H",
        "colab_type": "text"
      },
      "source": [
        "Plot Analysis result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX80X38Qnyrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(x, y, color='Red')\n",
        "plt.title('Sentiment Analysis', fontsize = 20)\n",
        "plt.xlabel('← Negative — — — — — — Positive →', fontsize=15)\n",
        "plt.ylabel('← Facts — — — — — — — Opinions →', fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp_gAdOdmdtI",
        "colab_type": "text"
      },
      "source": [
        "Github URL : https://github.com/lahirurane/UTS_ML2019_A2/blob/master/UTS_ML2019_A2_.ipynb"
      ]
    }
  ]
}